{"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy\n!pip install scipy\n!pip install seaborn\n!pip install pandas\n!pip install matplotlib\n!pip install sklearn\n!pip install pymatgen\n!pip install tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"95e9dc2f-16d5-4680-b4f7-fad3f1268ae5"},{"cell_type":"code","source":"# Importing Libraries\nimport numpy as np\nimport scipy as sp\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_predict, cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier, \\\n    GradientBoostingClassifier, GradientBoostingRegressor, \\\n    RandomForestClassifier, RandomForestRegressor\nfrom scipy import linalg\nfrom sklearn.discriminant_analysis import (LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA)\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom pymatgen.core import Element, Composition, periodic_table\nfrom functools import partial\nfrom pymatgen.ext.matproj import MPRester\nmpr = MPRester(\"241iWwhTEOaNmC6V\")\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\nfrom sklearn import svm, datasets\n# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# import tensorflow.keras.layers as layers\n# import tensorflow as tf\nfrom pandas.plotting import scatter_matrix\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e39dac21-8a87-48df-aa6a-799fd067c1af"},{"cell_type":"code","source":"#loading the testing and training data into the file to be manipulated in several forms\n\ntrain = pd.read_csv(\"train.csv\",index_col=False)\ntest = pd.read_csv(\"test.csv\",index_col=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1b6cbe1e-8089-480e-8f59-560e045057a6"},{"cell_type":"code","source":"#Using pymatgen and the material ID we can query some of the data in our training set that we will use AND we will store this in a dataframe\n\nbase_data = mpr.query(criteria={\"task_id\": {\"$in\":train[\"material_id\"].to_list()}}, properties=[\"material_id\",\"energy\",\n        \"energy_per_atom\",\n        \"volume\",\n        \"formation_energy_per_atom\",\n        \"nsites\",\n        \"pretty_formula\",                                                                                  \n        \"nelements\",\n        \"density\",  \"band_gap\"])\nbase_data_DF = pd.DataFrame(base_data)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ebe0b7ab-1efc-4581-bfcd-555e8a33e724"},{"cell_type":"code","source":"#for the training data we will remove noble gases. Later on we will have to apply fixes to our code because the test set includes Xe \n\n\n#Making list of possible noble gases \nnobles = [\"He\",\"Ne\", \"Ar\", \"Kr\", \"Xe\", \"Rn\", \"Og\"]\nNoNobles_base_data_DF = base_data_DF\n\n#Iterating through data and using pandas drop function to filter out materials containing noble gas elements \nfor i in nobles:\n    NoNobles_base_data_DF = NoNobles_base_data_DF.drop(NoNobles_base_data_DF[NoNobles_base_data_DF['pretty_formula'].str.contains(i)].index)\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2b5483cf-286e-4871-97fb-eb91ed7a98e3"},{"cell_type":"code","source":"#this isn't the most efficient way to do this but why fix somthing that isnt technically broken\n#I need to make an element properties dataframe similar to lab 2. How i did it in lab 2 was to get a list of unique elements in the NoNobles_base_data_DF and then making a dataframe that includes the data of each element\n\n\nlistA = [a for a in NoNobles_base_data_DF[\"pretty_formula\"]]\nlistB = [Composition(a) for a in listA]\nlistC = [a.elements for a in listB]\n\neditC = [item for sublist in listC for item in sublist]\nunique_editC = set(editC)\nunique_editC\n\nUL_editC = [a for a in unique_editC]\nindiv_ELproperties_train = [a.data for a in UL_editC]\n\npeel = set(UL_editC)\nsy_editC = [a.symbol for a in UL_editC]\nsy_editC\n\nindiv_ELproperties_train_DF = pd.DataFrame(indiv_ELproperties_train, index=sy_editC)\n\nindiv_ELproperties_train_DF.head()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"634834ea-2288-4c7d-9244-109c55c1cf50"},{"cell_type":"code","source":"#Off top there are some properties that I don't want to use, either because there is significant lack of data for them, they are mostly words and not numbers or they are ranges, or they seem to be repeated\n#there will be several droplists i will try to make this more ordered by numbering them all\n\ndroplist_1 = ['Ionic radii',\n    'Ionic radii hs', \n     'Ionic radii ls',\n     'iupac_ordering', \n     'IUPAC ordering', \n     'NMR Quadrupole Moment', \n     'Reflectivity',\n     'Refractive index', \n     'Rigidity modulus', \n     'Shannon radii',\n     'Superconduction temperature',\n     'Mendeleev no',       \n     'Mineral hardness',\n     'Molar volume',\n     'Name',\n     'Oxidation states',\n     'ICSD oxidation states',\n     'Brinell hardness',\n     'Atomic orbitals', \n     'Coefficient of linear thermal expansion',\n     'Atomic orbitals',\n     'Electronic structure',\n     'Electrical resistivity',\n     'Ground level'\n           ]\n\n\nindiv_ELproperties_train_DF = indiv_ELproperties_train_DF.drop(columns=droplist_1)\n\n\nindiv_ELproperties_train_DF.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"5957c8f6-2393-49cf-8e0e-e491a9308b35"},{"cell_type":"markdown","source":"So initially we only queried and used the data available on pymatgen without building any features like we did in lab 2 that model performance wasn't good. it was my first attempt and it had a value of 173 when I submitted it to kaggle. We talked to the TA and he said to build the features out. Now the .data command took us forever to find. I don't know why. But once we found that and had the values at least we could start to do data cleaning of the data we had enough of. But I did want to say this part took us SO SO long. It was honestly very frustrating and in the future i think it might be worth it to provide more examples of the data cleaning so that we can focus more on choosing features and models and not data cleaning.","metadata":{},"id":"1c899a6a-ce13-4ef1-9266-68405206dbb1"},{"cell_type":"code","source":"#Data cleaning lines. These should all work. But you can't run them twice without error. You are saving the new clean data into the dataframe so it wont be able to find anything since it is very column specific and once ran once\n##there is nothing to fix. that being said the coerce to apply numeric is in a different box because i dont want to risk it making them numeric before i do a bunch of string operations. \n\nindiv_ELproperties_train_DF['Boiling point'] = [(a.replace('K', '', 1)) for a in indiv_ELproperties_train_DF['Boiling point']]\nindiv_ELproperties_train_DF['Bulk modulus'] = indiv_ELproperties_train_DF[\"Bulk modulus\"].str.replace(\"GPa\", \"\")\nindiv_ELproperties_train_DF['Critical temperature'] = indiv_ELproperties_train_DF[\"Critical temperature\"].str.replace(\"K\", \"\")\nindiv_ELproperties_train_DF['Density of solid'] = indiv_ELproperties_train_DF['Density of solid'].str.replace(\"no data\", \"NaN\")\nindiv_ELproperties_train_DF['Density of solid'] = indiv_ELproperties_train_DF[\"Density of solid\"].str.replace(\"kg m<sup>-3</sup>\", \"\")\nindiv_ELproperties_train_DF['Liquid range'] = [(a.replace('K', '', 1)) for a in indiv_ELproperties_train_DF['Liquid range']]\nindiv_ELproperties_train_DF['Poissons ratio'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_train_DF['Poissons ratio']]\nindiv_ELproperties_train_DF['Poissons ratio'] = indiv_ELproperties_train_DF['Poissons ratio'].str.replace(\"no data\", \"\").astype(float)\nindiv_ELproperties_train_DF['Thermal conductivity'] = [float(a.replace('W m<sup>-1</sup> K<sup>-1</sup>', \"\", 1)) for a in indiv_ELproperties_train_DF['Thermal conductivity']]\nindiv_ELproperties_train_DF['Velocity of sound'] = indiv_ELproperties_train_DF['Velocity of sound'].str.replace(\"no data\",\"NaN\")\nindiv_ELproperties_train_DF['Velocity of sound'] = [(a.replace('m s<sup>-1</sup>', '', 1)) for a in indiv_ELproperties_train_DF['Velocity of sound']]\nindiv_ELproperties_train_DF['Vickers hardness'] = indiv_ELproperties_train_DF['Vickers hardness'].str.replace(\"no data\", \"NaN\")\nindiv_ELproperties_train_DF['Vickers hardness'] = [(a.replace('MN m<sup>-2</sup>', '', 1)) for a in indiv_ELproperties_train_DF['Vickers hardness']]\nindiv_ELproperties_train_DF['Youngs modulus'] = indiv_ELproperties_train_DF['Youngs modulus'].str.replace(\"no data\", \"NaN\")\nindiv_ELproperties_train_DF['Youngs modulus'] = [(a.replace('GPa', '', 1)) for a in indiv_ELproperties_train_DF['Youngs modulus']]\nindiv_ELproperties_train_DF['Bulk modulus'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_train_DF['Bulk modulus']]\nindiv_ELproperties_train_DF['Bulk modulus'] = [(a.replace('liquid', '', 1)) for a in indiv_ELproperties_train_DF['Bulk modulus']]\nindiv_ELproperties_train_DF['Bulk modulus'] = indiv_ELproperties_train_DF['Bulk modulus'].str.replace(r\"\\(.*\\)\",\"\",  regex=True).astype(float)\nindiv_ELproperties_train_DF['Melting point'] = indiv_ELproperties_train_DF['Melting point'].str.replace(\"K\", \"\")\nindiv_ELproperties_train_DF['Melting point'] = indiv_ELproperties_train_DF['Melting point'].str.replace(\"white P\", \"\")\nindiv_ELproperties_train_DF['Melting point'] = indiv_ELproperties_train_DF['Melting point'].str.replace(r\"\\(.*\\)\",\"\",  regex=True).astype(float)\nindiv_ELproperties_train_DF['Metallic radius'] = indiv_ELproperties_train_DF['Metallic radius'].astype(str)\nindiv_ELproperties_train_DF['Metallic radius'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_train_DF['Metallic radius']]\nindiv_ELproperties_train_DF['Metallic radius'] = indiv_ELproperties_train_DF['Metallic radius'].astype(float)\nindiv_ELproperties_train_DF['Common oxidation states'] = [len(a) for a in indiv_ELproperties_train_DF['Common oxidation states']]\nindiv_ELproperties_train_DF['First Ionization Energy'] = [a[0] for a in indiv_ELproperties_train_DF['Ionization energies']]\n\nindiv_ELproperties_train_DF = indiv_ELproperties_train_DF.drop(\"Ionization energies\", axis=1)\n\nindiv_ELproperties_train_DF['Critical temperature'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_train_DF['Critical temperature']]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f274021c-8d5c-4bc2-9f8f-11860ee7fb70"},{"cell_type":"code","source":"indiv_ELproperties_train_DF = indiv_ELproperties_train_DF.apply(pd.to_numeric, errors='coerce')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7b7c5859-18b0-45be-a85a-193fb30431c8"},{"cell_type":"code","source":"#We need to compute the mean values of each column so that way we can place the means of each column in the spaces where we previously made sure there was NAN\n\n#means\nmean_col_vals = dict(indiv_ELproperties_train_DF.mean())\nmean_col_vals\n\n\n# Iterating through variable with averages to replace the NaN values in element_data\nfor key, value in mean_col_vals.items():\n    indiv_ELproperties_train_DF.loc[indiv_ELproperties_train_DF[key].isnull(),key] = value\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"0ba4ac40-eae7-452f-8e7a-fe0ee5ac981c"},{"cell_type":"code","source":"#We talked with the TA about extensive vs intensive properties and we needed to remove volume and energy as a result. though we could keep volume/atom. however we would have to build that first. as it is just easier to build\n##that once we cleaned the other data. we did that here\n\n#NoNobles_base_data_DF\n\n\n\nNoNobles_basedata_DF_wcomp = NoNobles_base_data_DF\nNoNobles_basedata_DF_wcomp['Composition'] = [Composition(c) for c in NoNobles_basedata_DF_wcomp[\"pretty_formula\"]]\nNoNobles_basedata_DF_wcomp['num_atoms'] = [c.num_atoms for c in NoNobles_basedata_DF_wcomp['Composition']]\nNoNobles_basedata_DF_wcomp['volume_per_atom'] = NoNobles_basedata_DF_wcomp['volume']/NoNobles_basedata_DF_wcomp['num_atoms']\n\n\n\nNoNobles_basedata_DF_wcomp\n#this should be a dataframe of all the MPIDS with the relevant compositions which is necessary because we are about to start making features for the indiv_ELproperties_train_DF\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"0b38d74d-422a-471e-9ad9-e4e537a584d6"},{"cell_type":"code","source":"#this was the best way i figured out to make the additional properties. this is how i did it in lab 2. radius mean function was removed beceause it is never added to a dataframe and is just a tester to make sure they work\n\n\nindiv_ELproperties_train_DF_dict = indiv_ELproperties_train_DF.to_dict()\n\n#my functions\ndef propertymean(property, composition):\n    sumofproperty = 0\n    totalnumatoms = 0\n    for element, number in composition.items():\n        sumofproperty += (number*indiv_ELproperties_train_DF_dict[property][str(element)])\n        totalnumatoms += number\n    return sumofproperty/totalnumatoms\n\ndef maxofproperty(property, composition):\n    propmax = None\n    for element, number in composition.items():\n        propertyvalue = indiv_ELproperties_train_DF_dict[property][str(element)]\n        if propmax:\n            propmax = propertyvalue if propertyvalue > propmax else propmax\n        else:\n            propmax = propertyvalue\n    return propmax\n\ndef minofproperty(property, composition):\n    propmin = None\n    for element, number in composition.items():\n        propertyvalue = indiv_ELproperties_train_DF_dict[property][str(element)]\n        if propmin:\n            propmin = propertyvalue if propertyvalue < propmin else propmin\n        else:\n            propmin = propertyvalue\n    return propmin\n\n\n\n#assigning the values of those functions to a dataframe\n\navg_properties_df = pd.DataFrame()\n\nfor property in indiv_ELproperties_train_DF.columns:\n    individualpropertymean = partial(propertymean, property)\n    averages = NoNobles_basedata_DF_wcomp['Composition'].apply(individualpropertymean)\n    avg_properties_df[(\"average_\" + property)] = averages\n    \navg_properties_df.head()\nprint(\"Average properties Dimension: \", avg_properties_df.shape)\n\nmax_properties = pd.DataFrame()\n\nfor property in indiv_ELproperties_train_DF.columns:\n    individualpropertymax = partial(maxofproperty, property)\n    max = NoNobles_basedata_DF_wcomp['Composition'].apply(individualpropertymax)\n    max_properties[(\"max_\" + property)] = max\n    \nmin_properties = pd.DataFrame()\n\nfor property in indiv_ELproperties_train_DF.columns:\n    individualpropertymin = partial(minofproperty, property)\n    min = NoNobles_basedata_DF_wcomp['Composition'].apply(individualpropertymin)\n    min_properties[(\"min_\" + property)] = min\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ca96c662-b350-409e-a961-0f1e204dbd3d"},{"cell_type":"code","source":"#Now we need to take all theses dataframes we have made with the min/max/average properties and put them in a single matrix that the models will be performed on\n\n\nALL_Features_Matrix =  pd.concat([NoNobles_basedata_DF_wcomp, avg_properties_df, min_properties, max_properties], axis=1)\n\n#ALL_Features_Matrix.columns\n\n\n#this is the second droplist: this one is to get rid of some features that we needed before to create new features list, but that we cant use because we couldnt clean properly or we intensive properties\ndroplist_2 = ['volume','energy', 'pretty_formula', 'Composition', 'average_Common oxidation states', 'min_Common oxidation states','max_Common oxidation states']\n\n\nmodel_matrix_1 = ALL_Features_Matrix.drop(columns=droplist_2)\nmodel_matrix_1 \n#model_matrix_1 is the feature matrix for the training data that we will later split\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"9fab2f8a-a4de-48d7-87c3-68e829a41ef0"},{"cell_type":"code","source":"#We removed the noble gases from the feature matrix. so we have to make sure that the part of the training set that is just MPIDS and dielectric polytotals has those removed as well\n#While typing this i thought ... I removed noble gases from my training set. will that matter if one is included in my dataset.... something to think about for the next project. \n\n#list of the ids and a dataframe of them\nMPIDs = list(NoNobles_base_data_DF[\"material_id\"])\nMPIDs = pd.DataFrame(MPIDs)\n\n#setting a new dataframe equal to the dataframe that only includes the one in model_matrix_1 which does not include noble gases\n\nNoNobles_training_set = train[train.index.isin(MPIDs.index)] \n\n\n#you have to make the material_id the index because the dataframe operations can't handle strings\nNoNobles_training_set.set_index('material_id')\n\n#have to do the same to the feature matrix // idk why this one needs the inplace true but that took longer to get right that should have.\nmodel_matrix_1.set_index('material_id', inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2c3f5877-abc2-486d-a180-1f07ee6c92ad"},{"cell_type":"code","source":"#just in case i want to check what is in the NoNobles_training_set dataframe\n#NoNobles_training_set","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7e646a89-4770-4ef2-ba61-c30516570bec"},{"cell_type":"code","source":"#just in case i want to check what is in the NoNobles_training_set dataframe\n#model_matrix_1","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f312458f-ba92-4b98-8504-684fc5682ed3"},{"cell_type":"markdown","source":"Here is where you either run your model or start to do feature selection. I think for simplicity i will do the feature selection here and then do droplists according to different features I would like to drop.\nUltimately I didnt really like any of my models so it really doesn't matter which I choose. I have no clue how people got better than 7ish... which ultimately stephanie and I achieved working together even though i only submitted my 8 on kaggle. so i will have the feature selection first and then just do the splits for each model that i ultimately worked with \n\n\nsome other things we tried that didnt really improve the model accuracy was test size and random state.\n\nWith regards to feature selection.\n\ni tried a heat map (too many properties to see so I would up just printing the correlations to the datafram and looking at them there)\ni tried a scatter_matrix( never fully ran so I assume it was just too much data, so essentially same problem)\n\nI like printing the correlations to a dataframe and coloring those to determine which ones are important(it was brought up that this is low tech and could lead to missing them, but the others seemed like more work than they offered in results. (also additionally I never exactly figured out how to apply two colors to the same map) (you can idnore the ones that are 1 obviously)\n\n","metadata":{},"id":"0d428776-beba-4b59-8522-633f7d0372bc"},{"cell_type":"code","source":"#Getting the correlations\nmodel_matrix_1_corr = model_matrix_1.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"64444e54-0d58-4d46-b27c-9385a9fb16a1"},{"cell_type":"code","source":"#how to make the positive correlatins red\n\ndef color_negative_red(val):\n    color = 'red' if 0.85 < val and val < 1.00 else 'black'\n    return 'color: %s' % color\n\n\nmodel_matrix_1_corr.style.applymap(color_negative_red)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a5e07e29-d6c4-4858-9f81-3ba5a809e790"},{"cell_type":"code","source":"#how to make the negative correlatins blue\n\ndef color_negative_blue(val):\n    color = 'blue' if -1.00 < val and val < -0.85 else 'black'\n    return 'color: %s' % color\n\nmodel_matrix_1_corr.style.applymap(color_negative_blue)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f4fc95ed-65f3-4fd0-aea4-b1aa85941d0e"},{"cell_type":"code","source":"#need this block because you need to be able to see which ones are correlated without having to go through the dataframe which is kinda tedius\n\nlookthru = model_matrix_1_corr.columns\n\npos_correlated_ones_dict = {}\n\nfor col in lookthru:\n    var = model_matrix_1_corr[model_matrix_1_corr[col] > 0.85].index.tolist()\n    if col in var:\n        var.remove(col)\n    if var == []:\n        continue\n    pos_correlated_ones_dict[col] = var\nprint(pos_correlated_ones_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"dbc65732-a4f5-4479-a86d-19c1bd30b5c4"},{"cell_type":"markdown","source":"this is the subset of correlated ones you might remove on the positive correlation side\n\n{'average_Atomic mass': ['average_Atomic no'],  \n'average_Atomic radius': ['average_Atomic radius calculated', 'average_Van der waals radius'], , \n'average_Boiling point': ['average_Liquid range', 'average_Melting point'], , \n'average_Melting point': ['average_Boiling point'], \n'average_X': ['average_First Ionization Energy'], \n'average_Electron affinity': ['max_Electron affinity'], \n 'min_Atomic mass': ['min_Atomic no'], \n 'min_Atomic radius': ['min_Atomic radius calculated', 'min_Critical temperature', 'min_Van der waals radius'], \n 'min_Liquid range': ['min_Boiling point'], \n 'min_Melting point': ['min_Boiling point'],\n 'min_Van der waals radius': ['min_Atomic radius'], \n 'min_First Ionization Energy': ['min_X'], \n  'max_Atomic no': ['max_Atomic mass'], \n  'max_Atomic radius calculated': ['max_Atomic radius', 'max_Van der waals radius', 'max_Metallic radius'], \n  \n  \n  this is a subst of the correlated ones you might remove on the negative side\n  \n  average_Atomic radius': ['average_First Ionization Energy']\n  ","metadata":{},"id":"39db36c5-6101-47c5-ab31-e1e6810c1f84"},{"cell_type":"code","source":"#need this block because you need to be able to see which ones are correlated without having to go through the dataframe which is kinda tedius\n\nlookthru2 = model_matrix_1_corr.columns\n\nneg_correlated_ones_dict = {}\n\nfor col in lookthru2:\n    var2 = model_matrix_1_corr[model_matrix_1_corr[col] < -0.85].index.tolist()\n    if col in var2:\n        var2.remove(col)\n    if var2 == []:\n        continue\n    neg_correlated_ones_dict[col] = var2\nprint(neg_correlated_ones_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b72d5c39-ec9c-413c-8b58-ff45787171fc"},{"cell_type":"code","source":"# I dont really believe you need to run this. But I'll leave it in to show you i tried it. \n# this is the heat map to display the correlations but there are just too many here to sus out and i didnt know a better way than the one above. \n\nfig, ax = plt.subplots()\n## the size of A4 paper\nfig.set_size_inches(14, 10)\nsns.heatmap(model_matrix_1_corr)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"23932d05-feda-4bcf-8e7b-90a88be495a7"},{"cell_type":"code","source":"#This one i tried but i really want to leave this hashtagged because it took soooooo long to run and never completed but again leaving it in so that you know i tried\n\n\n# scatter_matrix(model_matrix_1, figsize=(20,20))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"c687685d-7361-461f-81f1-3f6619069835"},{"cell_type":"code","source":"#Feature Selection: this is a list of the features that that I will drop from model_matrix_1 based on the feature selection\n\ndroplist_3 = ['max_Van der waals radius','min_Liquid range','max_Atomic radius', 'max_Atomic radius calculated', 'max_Metallic radius']\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"852203a8-b567-424d-b4d0-65a30c66b9ae"},{"cell_type":"markdown","source":"Test Data Querying\n\nFor the sake of saving time. And because we could submit our files to kaggle to check if we are right. Though Ideally we would have kept training the model until we got very close to accurately predicting the Ytest_train data by using the model appropriately. But ultimately we just trained a model. Predicted the data of the actual test set and determined how close we were on kaggle. so there is not a real point to doing the actual model priming here when its not something that we actually did. so we can just query the the test data now. try to predict the values and see how close we get\n","metadata":{},"id":"4428a525-aa54-4fc3-8708-fddbf3730959"},{"cell_type":"code","source":"# Using material IDs provided in training data to get corresponding information from MPD\nbase_data_test = mpr.query(criteria={\"task_id\": {\"$in\":test[\"material_id\"].to_list()}}, properties=[\"material_id\",\"energy\",\n        \"energy_per_atom\",\n        \"volume\",\n        \"formation_energy_per_atom\",\n        \"nsites\",\n        \"pretty_formula\",                                                                                  \n        \"nelements\",\n        \"density\",  \"band_gap\"])\nbase_data_test_DF = pd.DataFrame(base_data_test)\n#display(base_data_test_DF)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"245c3e50-3a49-4a27-9591-439f9fd3bbc4"},{"cell_type":"code","source":"#Here is where for the Training data we removed the Noble Gases but here we will not be doing that so im not sure we actually need to call this anytype of filters\n\n\n\nlistA_X = [a for a in base_data_test_DF[\"pretty_formula\"]]\nlistB_X = [Composition(a) for a in listA_X]\nlistC_X = [a.elements for a in listB_X]\n\neditC_X = [item for sublist in listC_X for item in sublist]\nunique_editC_X = set(editC_X)\nunique_editC_X\n\nUL_editC_X = [a for a in unique_editC_X]\nindiv_ELproperties_test = [a.data for a in UL_editC_X]\n\npeel=set(UL_editC_X)\nsy_editC_X = [a.symbol for a in UL_editC_X]\nsy_editC_X\n\n#pd.set_option('display.max_rows', None)\n\n\nindiv_ELproperties_test_DF = pd.DataFrame(indiv_ELproperties_test, index=sy_editC_X)\n\n#Need droplist4 because you have to get rid of these two columns that exist because xenon is in the test group \ndroplist_4 = droplist_1 + ['Max oxidation state', 'Min oxidation state']\n\n#droplist_4\n\nindiv_ELproperties_test_DF = indiv_ELproperties_test_DF.drop(columns=droplist_4)\n#indiv_ELproperties_test_DF","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1aeec12d-373d-4702-aa5b-3e03a72d012f"},{"cell_type":"code","source":"#Data cleaning of the test data set\n\nindiv_ELproperties_test_DF['Boiling point'] = [(a.replace('K', '', 1)) for a in indiv_ELproperties_test_DF['Boiling point']]\nindiv_ELproperties_test_DF['Bulk modulus'] = indiv_ELproperties_test_DF[\"Bulk modulus\"].str.replace(\"GPa\", \"\")\nindiv_ELproperties_test_DF['Critical temperature'] = indiv_ELproperties_test_DF[\"Critical temperature\"].str.replace(\"K\", \"\")\nindiv_ELproperties_test_DF['Density of solid'] = indiv_ELproperties_test_DF['Density of solid'].str.replace(\"no data\", \"NaN\")\nindiv_ELproperties_test_DF['Density of solid'] = indiv_ELproperties_test_DF[\"Density of solid\"].str.replace(\"kg m<sup>-3</sup>\", \"\")\nindiv_ELproperties_test_DF['Liquid range'] = [(a.replace('K', '', 1)) for a in indiv_ELproperties_test_DF['Liquid range']]\nindiv_ELproperties_test_DF['Poissons ratio'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_test_DF['Poissons ratio']]\nindiv_ELproperties_test_DF['Poissons ratio'] = indiv_ELproperties_test_DF['Poissons ratio'].str.replace(\"no data\", \"\").astype(float)\nindiv_ELproperties_test_DF['Thermal conductivity'] = [float(a.replace('W m<sup>-1</sup> K<sup>-1</sup>', \"\", 1)) for a in indiv_ELproperties_test_DF['Thermal conductivity']]\nindiv_ELproperties_test_DF['Velocity of sound'] = indiv_ELproperties_test_DF['Velocity of sound'].str.replace(\"no data\",\"NaN\")\nindiv_ELproperties_test_DF['Velocity of sound'] = [(a.replace('m s<sup>-1</sup>', '', 1)) for a in indiv_ELproperties_test_DF['Velocity of sound']]\nindiv_ELproperties_test_DF['Vickers hardness'] = indiv_ELproperties_test_DF['Vickers hardness'].str.replace(\"no data\", \"NaN\", 1)\nindiv_ELproperties_test_DF['Vickers hardness'] = [(a.replace('MN m<sup>-2</sup>', '', 1)) for a in indiv_ELproperties_test_DF['Vickers hardness']]\nindiv_ELproperties_test_DF['Youngs modulus'] = indiv_ELproperties_test_DF['Youngs modulus'].str.replace(\"no data\", \"NaN\", 1)\nindiv_ELproperties_test_DF['Youngs modulus'] = [(a.replace('GPa', '', 1)) for a in indiv_ELproperties_test_DF['Youngs modulus']]\nindiv_ELproperties_test_DF['Bulk modulus'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_test_DF['Bulk modulus']]\nindiv_ELproperties_test_DF['Bulk modulus'] = [(a.replace('liquid', '', 1)) for a in indiv_ELproperties_test_DF['Bulk modulus']]\nindiv_ELproperties_test_DF['Bulk modulus'] = indiv_ELproperties_test_DF['Bulk modulus'].str.replace(r\"\\(.*\\)\",\"\",  regex=True).astype(float)\nindiv_ELproperties_test_DF['Melting point'] = indiv_ELproperties_test_DF['Melting point'].str.replace(\"K\", \"\")\nindiv_ELproperties_test_DF['Melting point'] = indiv_ELproperties_test_DF['Melting point'].str.replace(\"white P\", \"\")\nindiv_ELproperties_test_DF['Melting point'] = indiv_ELproperties_test_DF['Melting point'].str.replace(r\"\\(.*\\)\",\"\",  regex=True).astype(float)\nindiv_ELproperties_test_DF['Metallic radius'] = indiv_ELproperties_test_DF['Metallic radius'].astype(str)\nindiv_ELproperties_test_DF['Metallic radius'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_test_DF['Metallic radius']]\nindiv_ELproperties_test_DF['Metallic radius'] = indiv_ELproperties_test_DF['Metallic radius'].astype(float)\n#indiv_ELproperties_test_DF['Common oxidation states'] = [len(a) for a in indiv_ELproperties_test_DF['Common oxidation states']]\nindiv_ELproperties_test_DF['First Ionization Energy'] = [a[0] for a in indiv_ELproperties_test_DF['Ionization energies']]\n\nindiv_ELproperties_test_DF = indiv_ELproperties_test_DF.drop(\"Ionization energies\", axis=1)\n\nindiv_ELproperties_test_DF['Critical temperature'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_test_DF['Critical temperature']]\n\n#this code works but if for any reason there is an error you have to # out certain ones that cannot be run twice\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ac35f10a-623e-4b72-9e67-dfbc29d5fb3d"},{"cell_type":"code","source":"#indiv_ELproperties_test_DF['First Ionization Energy'] = [a[0] for a in indiv_ELproperties_test_DF['Ionization energies']]\n\n#indiv_ELproperties_test_DF = indiv_ELproperties_test_DF.drop(\"Ionization energies\", axis=1)\n\nindiv_ELproperties_test_DF['Critical temperature'] = [(a.replace('no data', 'NaN', 1)) for a in indiv_ELproperties_test_DF['Critical temperature']]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"c4b0d48c-f9af-4669-90e6-20f759df1841"},{"cell_type":"code","source":"indiv_ELproperties_test_DF['Atomic radius'] = indiv_ELproperties_test_DF[\"Atomic radius\"].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"db7554fc-cbc8-4fe5-a15c-5570214098ee"},{"cell_type":"code","source":"indiv_ELproperties_test_DF['Atomic radius'] = indiv_ELproperties_test_DF[\"Atomic radius\"].str.replace(\"no data\", \"NaN\", 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"04124232-1038-4c32-87d6-1f892cf4a1b5"},{"cell_type":"code","source":"indiv_ELproperties_test_DF","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"fffb7f64-4d6b-4b24-85c1-769fce3f2621"},{"cell_type":"code","source":"indiv_ELproperties_test_DF = indiv_ELproperties_test_DF.apply(pd.to_numeric, errors='coerce')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f47335d6-c2c1-40a4-a4f4-e288d49d097c"},{"cell_type":"code","source":"indiv_ELproperties_test_DF.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e8be0403-e891-4885-9300-ceb7b54b455d"},{"cell_type":"code","source":"#We need to compute the mean values of each column so that way we can place the means of each column in the spaces where we previously made sure there was NAN\n\n#means\nmean_col_vals_test = dict(indiv_ELproperties_test_DF.mean())\nmean_col_vals_test\n\n\n# Iterating through variable with averages to replace the NaN values in element_data\nfor key, value in mean_col_vals_test.items():\n    indiv_ELproperties_test_DF.loc[indiv_ELproperties_test_DF[key].isnull(),key] = value","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"48668405-3818-4427-81e0-c6355d8b30c4"},{"cell_type":"code","source":"indiv_ELproperties_test_DF.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"45c4394a-1566-43a3-8f52-c49b603ef724"},{"cell_type":"code","source":"#Adding some columns that we need \n\nbase_data_test_DF_wcomp = base_data_test_DF\nbase_data_test_DF_wcomp['Composition'] = [Composition(c) for c in base_data_test_DF_wcomp[\"pretty_formula\"]]\nbase_data_test_DF_wcomp['num_atoms'] = [c.num_atoms for c in base_data_test_DF_wcomp['Composition']]\nbase_data_test_DF_wcomp['volume_per_atom'] = base_data_test_DF_wcomp['volume']/base_data_test_DF_wcomp['num_atoms']\n#base_data_test_DF_wcomp","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"c35bb7bf-caae-4d0e-8a22-c64c8b0cff3a"},{"cell_type":"code","source":"\n\nindiv_ELproperties_test_DF_dict = indiv_ELproperties_test_DF.to_dict()\n\n\n#my functions\ndef propertymean_X(property, composition):\n    sumofproperty = 0\n    totalnumatoms = 0\n    for element, number in composition.items():\n        sumofproperty += (number*indiv_ELproperties_test_DF_dict[property][str(element)])\n        totalnumatoms += number\n    return sumofproperty/totalnumatoms\n\ndef maxofproperty_X(property, composition):\n    propmax = None\n    for element, number in composition.items():\n        propertyvalue = indiv_ELproperties_test_DF_dict[property][str(element)]\n        if propmax:\n            propmax = propertyvalue if propertyvalue > propmax else propmax\n        else:\n            propmax = propertyvalue\n    return propmax\n\ndef minofproperty_X(property, composition):\n    propmin = None\n    for element, number in composition.items():\n        propertyvalue = indiv_ELproperties_test_DF_dict[property][str(element)]\n        if propmin:\n            propmin = propertyvalue if propertyvalue < propmin else propmin\n        else:\n            propmin = propertyvalue\n    return propmin\n\n\n#assigning the values of those functions to a dataframe\n\navg_properties_df_X = pd.DataFrame()\n\nfor property in indiv_ELproperties_test_DF.columns:\n    individualpropertymean = partial(propertymean_X, property)\n    averages = base_data_test_DF_wcomp['Composition'].apply(individualpropertymean)\n    avg_properties_df_X[(\"average_\" + property)] = averages\n    \navg_properties_df_X.head()\nprint(\"Average properties Dimension: \", avg_properties_df_X.shape)\n\nmax_properties_X = pd.DataFrame()\n\nfor property in indiv_ELproperties_test_DF.columns:\n    individualpropertymax = partial(maxofproperty_X, property)\n    max = base_data_test_DF_wcomp['Composition'].apply(individualpropertymax)\n    max_properties_X[(\"max_\" + property)] = max\n    \nmin_properties_X = pd.DataFrame()\n\nfor property in indiv_ELproperties_test_DF.columns:\n    individualpropertymin = partial(minofproperty_X, property)\n    min = base_data_test_DF_wcomp['Composition'].apply(individualpropertymin)\n    min_properties_X[(\"min_\" + property)] = min\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1b8e3978-c401-450f-b1ff-47cda04ce057"},{"cell_type":"code","source":"base_data_test_DF_wcomp","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b2b90c6f-da59-40ee-bd96-ee2a2b41a44e"},{"cell_type":"code","source":"#model_matrix_2 is our feature space but for the test data\n\nALL_Features_Matrix_test = pd.concat([base_data_test_DF_wcomp, avg_properties_df_X, min_properties_X, max_properties_X], axis=1)\nALL_Features_Matrix_test.columns\n\n#droplist_2 hasnt changed from the one we used for our traindata\nmodel_matrix_2 = ALL_Features_Matrix_test.drop(columns=droplist_2)\n#model_matrix_2","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"1db602e0-da54-4799-90fc-7e8a740e76cb"},{"cell_type":"code","source":"#model_matrix_2","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ff467b3d-eeb3-4b83-aeb0-bde7cc47dec9"},{"cell_type":"code","source":"model_matrix_2.set_index('material_id',inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"6dc46cff-8701-4ed6-bb6c-d7e162566b0e"},{"cell_type":"code","source":"# testfornulls = model_matrix_2.isnull().sum()\n# testfornulls","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"83ea6c74-1a89-4477-9ae1-22f67f615d5f"},{"cell_type":"code","source":"#Train Test Split based on the training data\n\nXtrain_train, Xtest_train, Ytrain_train, Ytest_train = train_test_split(model_matrix_1, NoNobles_training_set,test_size=0.1, random_state=120)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b2c33810-cbd3-4e39-8bf9-2d28c6078438"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"51b1e509-fe55-43de-bfe4-f7f1383e998f"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"4109f29e-9531-4e58-a696-8bf9195e520b"},{"cell_type":"code","source":"NoNobles_training_set","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"c2b7e192-b200-45fa-8b16-5d3f64b161c4"},{"cell_type":"code","source":"#Normalizing Data\n\n# In some cases not normallizing the data provided better results. Is it always required to normalize data??\n\n# Computing mean and standard devaitaion for train X and normalizing\nmean_Xtrain = Xtrain_train.apply(np.mean, axis=0)\nstd_Xtrain = Xtrain_train.apply(np.std, axis=0)\nnorm_Xtrain = (Xtrain_train - mean_Xtrain) / std_Xtrain\n\n# Computing mean and standard devaitaion for test X and normalizing \nmean_Xtest = Xtest_train.apply(np.mean, axis=0)\nstd_Xtest = Xtest_train.apply(np.std, axis=0)\nnorm_Xtest = (Xtest_train - mean_Xtest) / std_Xtest\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"dabcec35-9ca8-4108-8c00-de704dafaa7d"},{"cell_type":"code","source":"norm_Xtrain","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"65fd3947-1130-41ff-818a-eb4cf21fbb78"},{"cell_type":"markdown","source":"Now we can do out models and provide results","metadata":{},"id":"9569bfe4-4cea-4888-8cad-c7e9a4356140"},{"cell_type":"markdown","source":"linear model 1 is on on all the features with normalized data","metadata":{},"id":"ba957b82-03bb-44f5-b370-adde3aa3db2d"},{"cell_type":"code","source":"linear_model_1 = LinearRegression()\n\nlinear_model_1.fit(norm_Xtrain, Ytrain_train['dielectric_poly_total'])\n#linear_model_1_predictions_traintest = linear_model_1.predict(norm_Xtrain)\nlinear_model_1_predictions_test = linear_model_1.predict(model_matrix_2)\n\nlinear_model_1_score = -cross_val_score(linear_model_1, norm_Xtrain, Ytrain_train['dielectric_poly_total'], cv=5, scoring='neg_mean_absolute_error')\n\n\n\nDF_linear_model_1 = pd.DataFrame(linear_model_1_predictions_test)\nlinear_model_1_list =[test, DF_linear_model_1]\n\nsubs =  pd.concat(linear_model_1_list, axis=1)\nsubmissions = subs.rename(columns={0:'dielectric_poly_total'})\nsubmissions.to_csv(\"linreg-1-attempt-1.csv\", index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8821033f-3011-462e-8365-494bace71557"},{"cell_type":"code","source":"np.mean(linear_model_1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f33ede2f-0bc9-48a5-9766-98f30c44e507"},{"cell_type":"code","source":"model_matrix_1","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"85484639-66bc-47c4-9977-3bfd736c9c97"},{"cell_type":"markdown","source":"linear model 2 is on on all the features with non normalized data","metadata":{},"id":"6f142674-5e1e-4171-9f00-afca33e22bd7"},{"cell_type":"code","source":"linear_model_2 = LinearRegression()\n\nlinear_model_2.fit(Xtrain_train, Ytrain_train['dielectric_poly_total'])\nlinear_model_2_predictions_traintest = linear_model_2.predict(Xtest_train)\nlinear_model_2_predictions_test = linear_model_2.predict(model_matrix_2)\n\nlinear_model_2_score = -cross_val_score(linear_model_2, Xtrain_train, Ytrain_train['dielectric_poly_total'], cv=5, scoring='neg_mean_absolute_error')\n\n\n\nDF_linear_model_2 = pd.DataFrame(linear_model_2_predictions_test)\nlinear_model_2_list =[test, DF_linear_model_2]\n\nsubs2 =  pd.concat(linear_model_2_list, axis=1)\nsubmissions2 = subs2.rename(columns={0:'dielectric_poly_total'})\nsubmissions2.to_csv(\"linreg-2-attempt-4.csv\", index=False)\n#needed four attempts because i kept printing linearmodel1 list into subs2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e1ff81a4-53d6-4663-b232-5236aa886aaf"},{"cell_type":"code","source":"np.mean(linear_model_2_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"176a6a7e-8485-4d2f-ab60-2e4e780a820c"},{"cell_type":"markdown","source":"basically the same\n\neven though the scores are the same if you print the next two cells you will see that the number predictions are very different","metadata":{},"id":"02d04018-864f-4d73-a30f-3d22db59aa0f"},{"cell_type":"code","source":"subs","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7c12f231-79d0-4829-be35-a1966aa15873"},{"cell_type":"markdown","source":"ridge_model_1 is with normalized data, i tried both normalized and non normalized they are equally bad","metadata":{},"id":"8b811ccb-139a-4425-b334-12042e97746f"},{"cell_type":"code","source":"ridgemodel_1 = Ridge(alpha=0.1, max_iter=-10000)\nridgemodel_1.fit(norm_Xtrain, Ytrain_train['dielectric_poly_total'])\n\nridgemodelprediction_1 = ridgemodel_1.predict(norm_Xtest)\nridgemodelprediction_1_test = ridgemodel_1.predict(model_matrix_2)\n\nridge_model_1_score = -cross_val_score(ridgemodel_1, norm_Xtrain, Ytrain_train['dielectric_poly_total'], cv=5, scoring='neg_mean_absolute_error')\n\nDF_ridge_model_1 = pd.DataFrame(ridgemodelprediction_1_test)\nridge_model_1_list = [test, DF_ridge_model_1]\n\n\n\nd_list_ridge = [test, DF_ridge_model_1]\n\n\nsubs_ridge = pd.concat(d_list_ridge, axis=1)\nsubmissions_ridge = subs_ridge.rename(columns={0:'dielectric_poly_total'})\nsubmissions_ridge.to_csv(\"ridge-1-attempt-1.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f97f58b0-c81e-48be-b09c-9c08b89d778f"},{"cell_type":"code","source":"np.mean(ridge_model_1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"294a8b80-b59d-469e-9830-8b93b1a4fbe0"},{"cell_type":"code","source":"subs_ridge","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a16f473e-dd93-4a2a-9b1c-62c6eea4ecb9"},{"cell_type":"markdown","source":"not good predicted values","metadata":{},"id":"d39e1687-6884-46bf-9143-ef8c4896c118"},{"cell_type":"code","source":"# logreg = LogisticRegression(penalty='none',solver='lbfgs', max_iter=1000)\n# logregmodel_1 = logreg.fit(norm_Xtrain, Ytrain_train['dielectric_poly_total'])\n\n# logregmodel_1_score = -cross_val_score(logregmodel_1, norm_train_X, y_train['dielectric_poly_total'], cv=5, scoring='neg_mean_absolute_error')\n\n# logreg_prediction_1_test = logregmodel_1.predict(model_matrix_2)\n\n# DF_logregmodel_1 = pd.DataFrame(logreg_prediction_1_test)\n\n# d_list_logreg = [test, DF_logregmodel_1]\n\n\n# subs_logreg = pd.concat(d_list_logreg, axis=1)\n# submissions_logreg = subs_logreg.rename(columns={0:'dielectric_poly_total'})\n# submissions_logreg.to_csv(\"logreg-1-attempt-1.csv\", index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7fc213c4-bf98-4b6f-bfbc-ca721276746d"},{"cell_type":"markdown","source":"got an error for the logreg model","metadata":{},"id":"96ff94c7-a01e-4234-8fca-60a4e6ff6d79"},{"cell_type":"markdown","source":"going to try the best model i have at this point with a major feature reduction\n","metadata":{},"id":"a7b5cddc-1bd6-4671-937d-0e545972afd4"},{"cell_type":"code","source":"droplist_5 = droplist_2 + ['average_Atomic mass', 'average_Atomic radius calculated', 'average_Van der waals radius','average_Liquid range', 'average_Melting point', 'average_First Ionization Energy',\n                           'max_Atomic mass', 'min_Atomic radius calculated', 'max_Van der waals radius','max_Liquid range', 'max_Melting point', 'max_First Ionization Energy',\n                          'min_Atomic mass', 'min_Atomic radius calculated', 'min_Van der waals radius','min_Liquid range', 'min_Melting point', 'min_First Ionization Energy', 'band_gap']\n\ndroplist_5","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"d3f4edaf-5c27-48cb-b6ef-1f654e1a8e71"},{"cell_type":"code","source":"#Now we need to take all theses dataframes we have made with the min/max/average properties and put them in a single matrix that the training models will be performed on\n\n\nALL_Features_Matrix =  pd.concat([NoNobles_basedata_DF_wcomp, avg_properties_df, min_properties, max_properties], axis=1)\n\n#ALL_Features_Matrix.columns\n\n\n#this is the second droplist: this one is to get rid of some features that we needed before to create new features list, but that we cant use because we couldnt clean properly or we intensive properties\n#droplist_2 = ['volume','energy', 'pretty_formula', 'Composition', 'average_Common oxidation states', 'min_Common oxidation states','max_Common oxidation states']\n#we are going to be using droplist_5 here because we want to make a training model that only had the data after our feature reduction\nmodel_matrix_3 = ALL_Features_Matrix.drop(columns=droplist_5)\nmodel_matrix_3 \n#model_matrix_1 is the feature matrix for the training data that we will later split","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f1fdb131-3023-45dc-924b-85df5f2c8753"},{"cell_type":"code","source":"#model_matrix_4 is our feature space but for the test data with the significant feature reduction\n\nALL_Features_Matrix_test = pd.concat([base_data_test_DF_wcomp, avg_properties_df_X, min_properties_X, max_properties_X], axis=1)\nALL_Features_Matrix_test.columns\n\n#droplist_2 hasnt changed from the one we used for our traindata\nmodel_matrix_4 = ALL_Features_Matrix_test.drop(columns=droplist_5)\nmodel_matrix_4","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2ca0e924-a527-472e-9909-28459608c7a1"},{"cell_type":"code","source":"model_matrix_3.set_index('material_id',inplace=True)\nmodel_matrix_4.set_index('material_id',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"f986583d-d1cf-48ba-a641-6b3656c126bc"},{"cell_type":"code","source":"#Train Test Split based on the training data\n\nXtrain_2, Xtest_2, Ytrain_2, Ytest_2 = train_test_split(model_matrix_3, NoNobles_training_set,test_size=0.1, random_state=120)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"c1fa3caa-ac3d-4610-88d3-42e330e4b792"},{"cell_type":"code","source":"linear_model_3 = LinearRegression()\n\nlinear_model_3.fit(Xtrain_2, Ytrain_2['dielectric_poly_total'])\nlinear_model_3_predictions_traintest = linear_model_3.predict(Xtest_2)\nlinear_model_3_predictions_test = linear_model_3.predict(model_matrix_4)\n\nlinear_model_3_score = -cross_val_score(linear_model_3, Xtrain_2, Ytrain_2['dielectric_poly_total'], cv=5, scoring='neg_mean_absolute_error')\n\n\n\nDF_linear_model_3 = pd.DataFrame(linear_model_3_predictions_test)\nlinear_model_3_list =[test, DF_linear_model_3]\n\nsubs3 =  pd.concat(linear_model_3_list, axis=1)\nsubmissions3 = subs3.rename(columns={0:'dielectric_poly_total'})\nsubmissions3.to_csv(\"linreg-3-attempt-1.csv\", index=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"97e048b4-d91d-4c52-889c-90bd516cb6e8"},{"cell_type":"code","source":"np.mean(linear_model_3_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"993e9be2-da9b-48d4-9ad3-c64ed2cbc0db"},{"cell_type":"markdown","source":"this isn't really any better  any better either. even after reducing features that were highly correlated down from 69 down to 51 and following the advice on piazza saying we shouldnt use bandgap\n\nbecause they aren't abundantly better im going to try some other models and see if it gets any better\n\n","metadata":{},"id":"f2448f90-cc58-4c7c-911d-0790419e4fda"},{"cell_type":"code","source":"#SVR model\n\nsvr_model_1 = svm.SVR(kernel='rbf', C=0.001, epsilon=0.001,  tol=1e-5)\nsvr_model_1.fit(Xtrain_train, Ytrain_train['dielectric_poly_total'])\n\n                      \nsvr_model_1_predictions_traintest = svr_model_1.predict(Xtest_train)\nsvr_model_1_predictions_test = svr_model_1.predict(model_matrix_2)\n                      \n\nsvr_model_1_score = -cross_val_score(svr_model_1, Xtrain_train, Ytrain_train['dielectric_poly_total'], cv=5, scoring='neg_mean_absolute_error')\n\n                      \nDF_svr_model_1 = pd.DataFrame(svr_model_1_predictions_test)                      \nsvr_model_1_list = [test, DF_svr_model_1]\n\nsubs4 = pd.concat(svr_model_1_list, axis=1)                      \nsubmissions4= subs4.rename(columns={0:'dielectric_poly_total'})\nsubmissions4.to_csv(\"svr-1-attempt-1.csv\", index=False)                      \n                      \n                      ","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"faed9b8a-6fb0-4445-b001-2854ea0cd98c"},{"cell_type":"code","source":"np.mean(svr_model_1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2b767239-62a8-4c03-9dbd-41fc616dabd3"},{"cell_type":"markdown","source":"#Random Forest Regressor\n\nrfr_model_1 = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=4 , random_state=42)\nrfr_model_1.fit(Xtrain_train, Ytrain_train['dielectric_poly_total'])\n\n                      \nrfr_model_1_predictions_traintest = rfr_model_1.predict(Xtest_train)\nrfr_model_1_predictions_test = rfr_model_1.predict(model_matrix_2)\n                      \n\nrfr_model_1_score = -cross_val_score(rfr_model_1, Xtrain_train, Ytrain_train['dielectric_poly_total'], cv=5, scoring='neg_mean_absolute_error')\n\n                      \nDF_rfr_model_1 = pd.DataFrame(rfr_model_1_predictions_test)                      \nrfr_model_1_list = [test, DF_rfr_model_1]\n\nsubs5 = pd.concat(rfr_model_1_list, axis=1)                      \nsubmissions5 = subs5.rename(columns={0:'dielectric_poly_total'})\nsubmissions5.to_csv(\"rfr-1-attempt-1.csv\", index=False)                      \n                      \n                      ","metadata":{},"id":"bbb7dbe4-1eba-4756-9519-4daaa045006e"},{"cell_type":"code","source":"np.mean(rfr_model_1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"fba9ae5d-d2fd-4171-81af-7fd84e204b70"},{"cell_type":"markdown","source":"###Everything below here is just stuff classmates tried that got them better data that i am confused about. Namely why you would query the exact id's for the test part of the training set. and also why i wasnt able to do so","metadata":{},"id":"dc83f22c-84c7-4fa4-98f7-508fae5a5f91"},{"cell_type":"code","source":"## Need a clean unindexed model_matrix 1 to work with that isnt called model_matrix_1\n\n\nmodel_matrix_5 = ALL_Features_Matrix.drop(columns=droplist_2)\nmodel_matrix_5\n#model_matrix_1 is the feature matrix for the training data that we will later split\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8ed4c1df-0ba3-487e-9b4d-84ef1d529654"},{"cell_type":"code","source":"mpids_2 = list(model_matrix_5[\"material_id\"])\nmpids_2 = pd.DataFrame(mpids_2)\n\n#setting a new dataframe equal to the dataframe that only includes the one in model_matrix_1 which does not include noble gases\n\nNoNobles_training_set_2 = train[train.index.isin(mpids_2.index)] \n\n\n#you have to make the material_id the index because the dataframe operations can't handle strings\nNoNobles_training_set_2.set_index('material_id')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"58c621ec-2cd2-4681-9008-95639cf469e3"},{"cell_type":"code","source":"model_matrix_5.set_index('material_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"81525f93-7fd0-41a7-8a06-11679a3c0552"},{"cell_type":"code","source":"#different style of train test split i did it like the way we did in class but others suggested this might be wrong\n\n#lists of  ids actually in the sets\ntrain_ids = list(NoNobles_training_set_2['material_id'])\ntest_ids = list(test['material_id'])\n\n\ntrain_X = model_matrix_1.loc[train_ids]\ntest_X = model_matrix_1.loc[test_ids]\n\ntrain_y = train['dielectric_poly_total']\n\ntest_y = mpr.query({'dielectric_poly_total': {\"$exists\": True}}, properties=['material_id', 'diel.poly_total'])\ntesy_y = pd.DataFrame(test_y)\ntest_y.set_index('material_id', inplace = True)\ntest_y = test_y.loc[test_ids]\ntest_y = test_y['diel.poly_total']\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e4fa8215-ae94-4275-9dd2-9cc7f2ba1c56"},{"cell_type":"code","source":"NoNobles_training_set\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"12f2dfbf-8da1-47a3-98f0-f8cb99287576"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"f525f4f7-72e2-4d62-b393-eb706f61ee4e"},{"cell_type":"code","source":"linear_model_4 = LinearRegression()\n\nlinear_model_4.fit(train_X, train_y)\nlinear_model_4_predictions_traintest = linear_model_4.predict(test_X)\nlinear_model_4_predictions_test = linear_model_4.predict(model_matrix_2)\n\nlinear_model_4_score = -cross_val_score(linear_model_4, train_X, train_y, cv=5, scoring='neg_mean_absolute_error')\n\n\n\nDF_linear_model_4 = pd.DataFrame(linear_model_4_predictions_test)\nlinear_model_4_list =[test, DF_linear_model_4]\n\nsubs4 =  pd.concat(linear_model_4_list, axis=1)\nsubmissions4 = subs4.rename(columns={0:'dielectric_poly_total'})\nsubmissions4.to_csv(\"linreg-4-attempt-1.csv\", index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"78a7757d-0459-4765-9f8c-511c8628d6ff"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"9dfb695b-eaf0-4782-a2a6-1c03c3b405f5"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"a291e1a7-dacb-40a9-b222-c795d0ea8c92"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"f5f9e998-5e92-43c4-8ad6-0c8f7b9547a7"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"674ab1ad-8b63-4cad-9281-2cead0c36597"}]}